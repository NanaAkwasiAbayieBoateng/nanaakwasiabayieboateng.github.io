<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Guide: Optimal Feature Removal</title>
    <style>
    html, body {
      margin: 0;
      padding: 0;
      height: 100%;
      width: 100%;
    }

    body {
      display: flex;
      justify-content: center;
      align-items: center;
      background-color: #282c34;
      color: white;
      font-family: sans-serif;
    }

    .content {
      text-align: center;
    }
  </style>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <!-- Application Structure Plan:
        The SPA is designed as a single-page application with a fixed sidebar navigation and a main content area. This structure allows users to easily navigate between different topics of the report.
        Sections include:
        1.  Overview: Introduces the problem and the solution's purpose.
        2.  Why Feature Selection?: Highlights benefits.
        3.  Key Evaluation Metrics: Explains Missing Data, Target Relationship, and Predictive Power with illustrative charts. This helps users understand the 'building blocks' of the decision.
        4.  The Composite Score: Details the formula and allows interactive adjustment of weights to see their impact on a hypothetical example. This promotes understanding of the scoring mechanism.
        5.  The Algorithm in Action: Shows the Python code and a conceptual flowchart of its steps, making the technical implementation transparent.
        6.  Interactive Explorer: A simplified demo where users input scores for two hypothetical correlated features and see which one would be kept based on the composite score and current weights. This provides a hands-on learning experience.
        7.  Advanced Tips: Summarizes best practices.
        8.  Conclusion: Key takeaways.
        This multi-section, guided approach was chosen to break down complex information into digestible parts, facilitating progressive understanding and engagement, rather than mirroring the linear structure of the source report.
    -->
    <!-- Visualization & Content Choices:
        - Overview: Text summary. Goal: Inform. Method: HTML text. Interaction: None. Justification: Sets context.
        - Why Feature Selection?: Bulleted list. Goal: Inform. Method: HTML list. Interaction: None. Justification: Quick benefits.
        - Key Evaluation Metrics (Missing Data, Target Relationship, Predictive Power): Each with text explanation and a Chart.js bar chart showing 3 example features. Goal: Inform/Compare. Method: HTML text + Chart.js Canvas. Interaction: Static charts. Justification: Concrete examples for abstract metrics.
        - The Composite Score: Text explanation of formula, interactive sliders for weights (JS), and a dynamically updated table showing example feature scores. Goal: Explain/Interact. Method: HTML text, input sliders, JS, HTML table. Interaction: User adjusts weights, table updates. Justification: Demonstrates impact of weighting.
        - The Algorithm in Action: Python code in `<pre>` block, conceptual flowchart (HTML/CSS styled list/divs). Goal: Explain. Method: HTML. Interaction: None. Justification: Shows implementation details.
        - Interactive Explorer: User inputs for 2 features (missing, target corr, predictive power - all 0-1 range for simplicity), "Calculate" button. JS calculates composite scores using current weights and displays results in a table, highlighting the feature to keep. Goal: Engage/Apply. Method: HTML forms, JS, HTML table. Interaction: User input and calculation. Justification: Hands-on application of concepts.
        - Advanced Tips: Bulleted list. Goal: Inform. Method: HTML list. Interaction: None. Justification: Practical advice.
        - Conclusion: Text summary. Goal: Summarize. Method: HTML text. Interaction: None. Justification: Key takeaways.
        - Libraries: Chart.js for all charts (Canvas). Tailwind for all styling. Vanilla JS for all logic.
        - CONFIRMATION: NO SVG graphics used. NO Mermaid JS used.
    -->
    <style>
        body { font-family: 'Inter', sans-serif; }
        .content-section { display: none; }
        .content-section.active { display: block; }
        .chart-container { position: relative; width: 100%; max-width: 600px; margin-left: auto; margin-right: auto; height: 300px; max-height: 400px; }
        @media (min-width: 768px) { .chart-container { height: 350px; } }
        .sidebar-link { display: block; padding: 0.75rem 1rem; margin-bottom: 0.5rem; border-radius: 0.375rem; transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out; }
        .sidebar-link:hover { background-color: #E5E7EB; color: #1F2937; } /* stone-200, stone-800 */
        .sidebar-link.active-link { background-color: #0D9488; color: white; } /* teal-600 */
        .slider-label { display: block; margin-bottom: 0.25rem; font-size: 0.875rem; color: #4B5563; } /* stone-600 */
        .slider-value { font-weight: 500; color: #0D9488; } /* teal-600 */
        .code-block { background-color: #1F2937; color: #D1D5DB; padding: 1rem; border-radius: 0.5rem; overflow-x: auto; font-family: 'Courier New', Courier, monospace; font-size: 0.875rem; line-height: 1.4; }
        .code-block .keyword { color: #93C5FD; } /* blue-300 */
        .code-block .comment { color: #6B7280; } /* stone-500 */
        .code-block .string { color: #A7F3D0; } /* green-200 */
        .code-block .function { color: #FDE047; } /* yellow-300 */
        .code-block .number { color: #F9A8D4; } /* pink-300 */
        .info-box { background-color: #EFF6FF; border-left: 4px solid #3B82F6; padding: 1rem; margin-bottom: 1rem; border-radius: 0.25rem; } /* blue-50 bg, blue-500 border */
        .info-box p { color: #1E40AF; } /* blue-800 */
        .pill { display: inline-block; padding: 0.25rem 0.75rem; font-size: 0.75rem; font-weight: 500; border-radius: 9999px; }
        .pill-keep { background-color: #D1FAE5; color: #065F46; } /* green-100 bg, green-700 text */
        .pill-remove { background-color: #FEE2E2; color: #991B1B; } /* red-100 bg, red-700 text */
    </style>
</head>
<body class="bg-stone-100 text-stone-800">
    <div class="flex flex-col md:flex-row min-h-screen">
        <aside class="w-full md:w-64 bg-white shadow-md p-4 space-y-2 md:sticky md:top-0 md:h-screen overflow-y-auto">
            <h1 class="text-2xl font-bold text-teal-700 mb-6 border-b pb-2">Feature Selector Guide</h1>
            <nav id="sidebarNav">
                <a href="#overview" class="sidebar-link active-link">Overview</a>
                <a href="#why-selection" class="sidebar-link">Why Feature Selection?</a>
                <div>
                    <span class="font-semibold text-stone-600 px-4 py-2 text-sm">Key Metrics</span>
                    <a href="#metric-missing" class="sidebar-link ml-4">Missing Data</a>
                    <a href="#metric-target" class="sidebar-link ml-4">Target Relationship</a>
                    <a href="#metric-predictive" class="sidebar-link ml-4">Predictive Power</a>
                </div>
                <a href="#composite-score" class="sidebar-link">The Composite Score</a>
                <a href="#algorithm" class="sidebar-link">Algorithm in Action</a>
                <a href="#explorer" class="sidebar-link">Interactive Explorer</a>
                <a href="#advanced-tips" class="sidebar-link">Advanced Tips</a>
                <a href="#conclusion" class="sidebar-link">Conclusion</a>
            </nav>
        </aside>

        <main class="flex-1 p-6 md:p-10 overflow-y-auto">
            <section id="overview" class="content-section active">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">Optimal Feature Removal: An Interactive Guide</h2>
                <p class="mb-4 text-lg leading-relaxed">
                    Welcome! This guide explores a sophisticated method for removing highly correlated features from a dataset. In machine learning, dealing with multicollinearity (where features are highly correlated) is crucial for building robust and interpretable models.
                </p>
                <p class="mb-4 leading-relaxed">
                    Traditional methods often remove one feature from a correlated pair arbitrarily. This interactive application, based on the "Optimal Python Function for Highly Correlated Feature Removal" report, introduces a multi-criteria approach. It uses a composite score based on:
                </p>
                <ul class="list-disc list-inside mb-4 pl-4 space-y-1 leading-relaxed">
                    <li>Proportion of missing data in a feature.</li>
                    <li>Strength of a feature's correlation with the target variable.</li>
                    <li>Individual predictive power of a feature (e.g., Lift Ratio for classification, R-squared for regression).</li>
                </ul>
                <p class="leading-relaxed">
                    Navigate through the sections using the sidebar to understand the metrics, see how the composite score is calculated, explore the Python algorithm, and even experiment with a simplified interactive demo.
                </p>
            </section>

            <section id="why-selection" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">Why is Feature Selection Important?</h2>
                <p class="mb-4 leading-relaxed">
                    Feature selection is a vital preprocessing step in machine learning. Its main goal is to identify and keep only the most relevant features from your dataset. This process offers several significant benefits:
                </p>
                <ul class="list-disc list-inside mb-4 pl-4 space-y-2 leading-relaxed bg-stone-50 p-6 rounded-lg shadow">
                    <li><strong class="text-teal-600">Enhances Model Efficacy:</strong> By removing irrelevant or redundant features, models can often achieve better predictive accuracy and generalize better to new, unseen data.</li>
                    <li><strong class="text-teal-600">Reduces Computational Overhead:</strong> Fewer features mean less data to process, leading to faster model training and prediction times. This is especially important for large datasets.</li>
                    <li><strong class="text-teal-600">Improves Model Interpretability:</strong> Models with fewer features are generally easier to understand and explain. It becomes clearer which factors are driving the predictions.</li>
                    <li><strong class="text-teal-600">Mitigates Overfitting:</strong> Redundant or irrelevant features can sometimes cause models to "memorize" the training data (overfit) rather than learning general patterns, leading to poor performance on new data.</li>
                    <li><strong class="text-teal-600">Addresses Multicollinearity:</strong> Specifically, removing highly correlated features (which provide similar information) helps stabilize model coefficients and makes it easier to understand the individual contribution of each feature.</li>
                </ul>
                <p class="leading-relaxed">
                    Ultimately, effective feature selection leads to more efficient, robust, and understandable machine learning models.
                </p>
            </section>

            <section id="metric-missing" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">Key Metric: Missing Data Proportion</h2>
                <p class="mb-4 leading-relaxed">
                    The proportion of missing values in a feature is a direct indicator of its completeness and reliability. Features with a high percentage of missing data are generally less informative and can complicate model training, even if imputation techniques are used.
                </p>
                <p class="mb-4 leading-relaxed">
                    <strong>Calculation:</strong> For each feature, it's `(Number of Null/NaN values) / (Total number of rows)`.
                </p>
                <p class="mb-6 leading-relaxed">
                    In the composite removal score, a <strong class="text-amber-600">higher missing proportion makes a feature more likely to be removed</strong>, as it suggests lower data quality and reliability.
                </p>
                <div class="chart-container bg-white p-4 rounded-lg shadow-lg">
                    <canvas id="missingDataChart"></canvas>
                </div>
                <p class="text-center mt-2 text-sm text-stone-600">Illustrative example of missing data proportions for hypothetical features.</p>
            </section>

            <section id="metric-target" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">Key Metric: Feature-Target Relationship</h2>
                <p class="mb-4 leading-relaxed">
                    The strength of a feature's relationship with the target variable is crucial. Features that strongly predict or associate with the target are more valuable. The specific statistical measure used depends on the data types of the feature and the target (e.g., Pearson correlation for numeric-numeric, ANOVA F-value for numeric-categorical, Mutual Information for mixed types).
                </p>
                <p class="mb-6 leading-relaxed">
                    After calculating the appropriate metric, its value (often the absolute value) is normalized. In the composite removal score, a <strong class="text-amber-600">stronger (higher) normalized relationship with the target makes a feature less likely to be removed</strong>.
                </p>
                <div class="chart-container bg-white p-4 rounded-lg shadow-lg">
                    <canvas id="targetRelationshipChart"></canvas>
                </div>
                <p class="text-center mt-2 text-sm text-stone-600">Illustrative example of normalized target relationship scores.</p>
            </section>

            <section id="metric-predictive" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">Key Metric: Individual Predictive Power</h2>
                <p class="mb-4 leading-relaxed">
                    This metric assesses how well a single feature, on its own, can predict the target variable. For classification tasks, this is often represented by the Lift Ratio, calculated from a simple model trained only on that feature. For regression tasks, R-squared from a simple linear regression can be used.
                </p>
                <p class="mb-6 leading-relaxed">
                    A higher lift (or R-squared) indicates greater individual predictive capability. In the composite removal score, <strong class="text-amber-600">higher normalized predictive power makes a feature less likely to be removed</strong>.
                </p>
                <div class="chart-container bg-white p-4 rounded-lg shadow-lg">
                    <canvas id="predictivePowerChart"></canvas>
                </div>
                <p class="text-center mt-2 text-sm text-stone-600">Illustrative example of normalized individual predictive power scores.</p>
            </section>

            <section id="composite-score" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">The Composite Removal Score</h2>
                <p class="mb-4 leading-relaxed">
                    To make an informed decision about which feature to remove from a highly correlated group, individual scores (missing proportion, target correlation, predictive power) are combined into a single composite removal score. These individual scores are first normalized (typically to a 0-1 range).
                </p>
                <p class="mb-2 leading-relaxed font-medium">The formula is structured as a weighted sum:</p>
                <div class="code-block mb-6 text-sm">
                    <code>Removal_Score = (W<sub>missing</sub> * Norm_Missing) + (W<sub>corr</sub> * (1 - Norm_Target_Corr)) + (W<sub>lift</sub> * (1 - Norm_Predictive_Power))</code>
                </div>
                <p class="mb-4 leading-relaxed">
                    Where:
                </p>
                <ul class="list-disc list-inside mb-4 pl-4 space-y-1 leading-relaxed">
                    <li><code>W<sub>missing</sub>, W<sub>corr</sub>, W<sub>lift</sub></code> are user-defined weights (summing to 1).</li>
                    <li><code>Norm_Missing</code> is the normalized missing proportion.</li>
                    <li><code>(1 - Norm_Target_Corr)</code> inverts the normalized target correlation (higher original correlation means lower contribution to removal score).</li>
                    <li><code>(1 - Norm_Predictive_Power)</code> inverts the normalized predictive power (higher original power means lower contribution to removal score).</li>
                </ul>
                <p class="mb-6 leading-relaxed">
                    A <strong class="text-amber-600">higher final Removal_Score indicates the feature is a better candidate for removal</strong>. The feature with the lowest score in a correlated group is kept.
                </p>

                <div class="bg-white p-6 rounded-lg shadow-lg">
                    <h3 class="text-xl font-semibold mb-4 text-teal-600">Adjust Weights & See Impact:</h3>
                    <div class="grid grid-cols-1 md:grid-cols-3 gap-6 mb-6">
                        <div>
                            <label for="missingWeight" class="slider-label">Missing Data Weight (<span id="missingWeightValue" class="slider-value">0.3</span>):</label>
                            <input type="range" id="missingWeight" min="0" max="1" step="0.01" value="0.3" class="w-full h-2 bg-stone-200 rounded-lg appearance-none cursor-pointer accent-teal-600">
                        </div>
                        <div>
                            <label for="targetCorrWeight" class="slider-label">Target Correlation Weight (<span id="targetCorrWeightValue" class="slider-value">0.4</span>):</label>
                            <input type="range" id="targetCorrWeight" min="0" max="1" step="0.01" value="0.4" class="w-full h-2 bg-stone-200 rounded-lg appearance-none cursor-pointer accent-teal-600">
                        </div>
                        <div>
                            <label for="liftWeight" class="slider-label">Predictive Power Weight (<span id="liftWeightValue" class="slider-value">0.3</span>):</label>
                            <input type="range" id="liftWeight" min="0" max="1" step="0.01" value="0.3" class="w-full h-2 bg-stone-200 rounded-lg appearance-none cursor-pointer accent-teal-600">
                        </div>
                    </div>
                    <div class="info-box mb-6">
                        <p>Note: The weights above will be normalized to sum to 1 for the calculation if they don't already.</p>
                    </div>

                    <h4 class="text-lg font-semibold mb-3">Example Calculation:</h4>
                    <table class="min-w-full divide-y divide-stone-200 border border-stone-300 rounded-md">
                        <thead class="bg-stone-50">
                            <tr>
                                <th class="px-4 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Feature</th>
                                <th class="px-4 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Norm. Missing (Higher=Worse)</th>
                                <th class="px-4 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Norm. Target Corr (Higher=Better)</th>
                                <th class="px-4 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Norm. Predictive Power (Higher=Better)</th>
                                <th class="px-4 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Composite Removal Score (Lower=Better)</th>
                            </tr>
                        </thead>
                        <tbody class="bg-white divide-y divide-stone-200" id="compositeScoreTableBody">
                            </tbody>
                    </table>
                </div>
            </section>

            <section id="algorithm" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">The Algorithm in Action</h2>
                <p class="mb-4 leading-relaxed">
                    The core logic is encapsulated in a Python function, `remove_correlated_features_optimal`. Here's a conceptual overview of its steps:
                </p>
                <div class="space-y-6 bg-white p-6 rounded-lg shadow-lg">
                    <div class="flex items-start">
                        <div class="flex-shrink-0 w-12 h-12 bg-teal-500 text-white rounded-full flex items-center justify-center text-xl font-bold mr-4">1</div>
                        <div>
                            <h4 class="font-semibold text-lg text-teal-600">Calculate Individual Scores</h4>
                            <p class="text-stone-700">For every feature: compute missing proportion, feature-target relationship score (adapting to data types), and individual predictive power (Lift/R-squared).</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="flex-shrink-0 w-12 h-12 bg-teal-500 text-white rounded-full flex items-center justify-center text-xl font-bold mr-4">2</div>
                        <div>
                            <h4 class="font-semibold text-lg text-teal-600">Identify Correlated Groups</h4>
                            <p class="text-stone-700">Using Pearson correlation (for numerical features), build a graph where nodes are features and edges connect highly correlated pairs (above a threshold). Connected components in this graph form the correlated groups.</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="flex-shrink-0 w-12 h-12 bg-teal-500 text-white rounded-full flex items-center justify-center text-xl font-bold mr-4">3</div>
                        <div>
                            <h4 class="font-semibold text-lg text-teal-600">Normalize Scores Globally</h4>
                            <p class="text-stone-700">Normalize all missing proportions, target correlations, and predictive power scores (e.g., Min-Max scaling to 0-1 range) across all features to ensure fair comparison.</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="flex-shrink-0 w-12 h-12 bg-teal-500 text-white rounded-full flex items-center justify-center text-xl font-bold mr-4">4</div>
                        <div>
                            <h4 class="font-semibold text-lg text-teal-600">Calculate Composite Score & Decide</h4>
                            <p class="text-stone-700">For each feature within a correlated group, calculate its composite removal score using the weighted formula. The feature with the lowest composite score in the group is kept; others are marked for removal.</p>
                        </div>
                    </div>
                     <div class="flex items-start">
                        <div class="flex-shrink-0 w-12 h-12 bg-teal-500 text-white rounded-full flex items-center justify-center text-xl font-bold mr-4">5</div>
                        <div>
                            <h4 class="font-semibold text-lg text-teal-600">Remove Features</h4>
                            <p class="text-stone-700">Drop all marked features from the DataFrame.</p>
                        </div>
                    </div>
                </div>

                <h3 class="text-2xl font-semibold mt-10 mb-4 text-teal-700">Python Function Snippet</h3>
                <p class="mb-4 leading-relaxed">
                    The following is a condensed representation of the Python function structure.
                </p>
                <div class="code-block">

 <pre class="code-block"><code class="language-python">
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler
<span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression, LinearRegression
<span class="keyword">from</span> mlxtend.evaluate <span class="keyword">import</span> lift_score
<span class="keyword">import</span> networkx <span class="keyword">as</span> nx

<span class="keyword">def</span> <span class="function">remove_correlated_features_optimal</span>(
    df: pd.DataFrame,
    target_column: <span class="string">str</span>,
    corr_threshold: <span class="keyword">float</span> = <span class="number">0.9</span>,
    missing_weight: <span class="keyword">float</span> = <span class="number">0.3</span>,
    target_corr_weight: <span class="keyword">float</span> = <span class="number">0.4</span>,
    lift_weight: <span class="keyword">float</span> = <span class="number">0.3</span>,
) -> pd.DataFrame:
    <span class="string">"""
    Removes highly correlated features based on a composite score.
    Considers missing proportion, target correlation, and predictive power.
    """</span>
    df_copy = df.copy()
    features = [col <span class="keyword">for</span> col <span class="keyword">in</span> df_copy.columns <span class="keyword">if</span> col != target_column]
    
    <span class="comment"># --- 1. Calculate All Individual Feature Scores ---</span>
    feature_scores = {}
    
    <span class="comment"># Calculate missing proportion for each feature</span>
    missing_props = df_copy[features].isna().mean()
    
    <span class="comment"># Calculate target correlation for numerical features</span>
    numerical_features = df_copy[features].select_dtypes(include=np.number).columns.tolist()
    target = df_copy[target_column]
    
    <span class="comment"># Calculate lift score (predictive power) using logistic regression</span>
    lift_scores = {}
    <span class="keyword">for</span> col <span class="keyword">in</span> numerical_features:
        <span class="keyword">try</span>:
            model = LogisticRegression() <span class="keyword">if</span> target.dtype == np.dtype(<span class="string">'O'</span>) <span class="keyword">else</span> LinearRegression()
            model.fit(df_copy[[col]].fillna(<span class="number">0</span>), target)
            y_pred = model.predict(df_copy[[col]].fillna(<span class="number">0</span>))
            lift_scores[col] = lift_score(target, y_pred)
        <span class="keyword">except</span>:
            lift_scores[col] = <span class="number">0</span>
    
    <span class="comment"># Gather all scores</span>
    <span class="keyword">for</span> col <span class="keyword">in</span> features:
        <span class="keyword">try</span>:
            missing_prop = missing_props[col]
            target_corr = np.abs(df_copy[[col, target_column]].dropna().corr().iloc[<span class="number">0</span>, <span class="number">1</span>]) <span class="keyword">if</span> col <span class="keyword">in</span> numerical_features <span class="keyword">else</span> <span class="number">0</span>
            pred_power = lift_scores.get(col, <span class="number">0</span>)
            
            feature_scores[col] = {
                <span class="string">'missing_prop'</span>: missing_prop,
                <span class="string">'target_corr'</span>: target_corr,
                <span class="string">'pred_power'</span>: pred_power
            }
        <span class="keyword">except</span>:
            feature_scores[col] = {
                <span class="string">'missing_prop'</span>: <span class="number">1</span>,
                <span class="string">'target_corr'</span>: <span class="number">0</span>,
                <span class="string">'pred_power'</span>: <span class="number">0</span>
            }
    
    <span class="comment"># Normalize scores globally</span>
    missing_vals = np.array([score[<span class="string">'missing_prop'</span>] <span class="keyword">for</span> score <span class="keyword">in</span> feature_scores.values()])
    target_corr_vals = np.array([score[<span class="string">'target_corr'</span>] <span class="keyword">for</span> score <span class="keyword">in</span> feature_scores.values()])
    pred_power_vals = np.array([score[<span class="string">'pred_power'</span>] <span class="keyword">for</span> score <span class="keyword">in</span> feature_scores.values()])
    
    scaler = MinMaxScaler()
    missing_vals_norm = scaler.fit_transform(missing_vals.reshape(-<span class="number">1</span>, <span class="number">1</span>)).flatten()
    target_corr_vals_norm = scaler.fit_transform(target_corr_vals.reshape(-<span class="number">1</span>, <span class="number">1</span>)).flatten()
    pred_power_vals_norm = scaler.fit_transform(pred_power_vals.reshape(-<span class="number">1</span>, <span class="number">1</span>)).flatten()
    
    <span class="comment"># --- 2. Identifying and Grouping Correlated Features ---</span>
    corr_matrix = df_copy[numerical_features].corr().abs()
    G = nx.Graph()
    
    <span class="comment"># Build graph based on correlation matrix</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="function">range</span>(<span class="function">len</span>(corr_matrix)):
        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="function">range</span>(i+<span class="number">1</span>, <span class="function">len</span>(corr_matrix)):
            <span class="keyword">if</span> corr_matrix.iloc[i, j] > corr_threshold:
                G.add_edge(corr_matrix.index[i], corr_matrix.index[j])
    
    correlated_groups = <span class="function">list</span>(nx.connected_components(G))
    
    <span class="comment"># --- 3. Applying the Composite Score within Correlated Groups ---</span>
    features_to_drop = <span class="function">set</span>()
    
    <span class="keyword">for</span> group <span class="keyword">in</span> correlated_groups:
        <span class="keyword">if</span> <span class="function">len</span>(group) <= <span class="number">1</span>:
            <span class="keyword">continue</span>
        group_feature_data = []
        
        <span class="comment"># Get normalized scores for features in this group</span>
        <span class="keyword">for</span> feature <span class="keyword">in</span> group:
            norm_missing = missing_vals_norm[[col == feature <span class="keyword">for</span> col <span class="keyword">in</span> features]]
            norm_target_corr = target_corr_vals_norm[[col == feature <span class="keyword">for</span> col <span class="keyword">in</span> features]]
            norm_pred_power = pred_power_vals_norm[[col == feature <span class="keyword">for</span> col <span class="keyword">in</span> features]]
            
            composite_score = (
                missing_weight * norm_missing +
                target_corr_weight * (<span class="number">1</span> - norm_target_corr) +
                lift_weight * (<span class="number">1</span> - norm_pred_power)
            )
            
            group_feature_data.append({<span class="string">'name'</span>: feature, <span class="string">'score'</span>: composite_score[<span class="number">0</span>]})  <span class="comment"># Fixed indexing</span>
            
        <span class="comment"># Sort features by composite score and drop all but the best one</span>
        sorted_group = <span class="function">sorted</span>(group_feature_data, key=<span class="keyword">lambda</span> x: x[<span class="string">'score'</span>])
        features_to_drop.update([f[<span class="string">'name'</span>] <span class="keyword">for</span> f <span class="keyword">in</span> sorted_group[<span class="number">1</span>:]])
    
    final_df = df_copy.drop(columns=<span class="function">list</span>(features_to_drop))
    <span class="keyword">return</span> final_df
</code></pre>

                </div>
            </section>

            <section id="explorer" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">Interactive Explorer</h2>
                <p class="mb-4 leading-relaxed">
                    Experiment with how the composite score changes for two hypothetical correlated features. Adjust their scores and the weights (using sliders in "The Composite Score" section) to see which feature would be kept.
                </p>
                <p class="mb-6 leading-relaxed">
                    For simplicity in this demo, input scores between 0 and 1 for Missing Proportion, Target Correlation, and Predictive Power. A higher Missing Proportion is worse, while higher Target Correlation and Predictive Power are better.
                </p>

                <div class="bg-white p-6 rounded-lg shadow-lg">
                    <div class="grid grid-cols-1 md:grid-cols-2 gap-8 mb-6">
                        <div>
                            <h4 class="text-lg font-semibold mb-3 text-teal-600">Feature 1 Details:</h4>
                            <div class="space-y-3">
                                <div>
                                    <label for="feat1Name" class="block text-sm font-medium text-stone-700">Name:</label>
                                    <input type="text" id="feat1Name" value="Feature A" class="mt-1 block w-full px-3 py-2 border border-stone-300 rounded-md shadow-sm focus:outline-none focus:ring-teal-500 focus:border-teal-500 sm:text-sm">
                                </div>
                                <div>
                                    <label for="feat1Missing" class="block text-sm font-medium text-stone-700">Missing Proportion (0-1, higher=worse):</label>
                                    <input type="number" id="feat1Missing" step="0.01" min="0" max="1" value="0.1" class="mt-1 block w-full px-3 py-2 border border-stone-300 rounded-md shadow-sm focus:outline-none focus:ring-teal-500 focus:border-teal-500 sm:text-sm">
                                </div>
                                <div>
                                    <label for="feat1TargetCorr" class="block text-sm font-medium text-stone-700">Target Correlation (0-1, higher=better):</label>
                                    <input type="number" id="feat1TargetCorr" step="0.01" min="0" max="1" value="0.8" class="mt-1 block w-full px-3 py-2 border border-stone-300 rounded-md shadow-sm focus:outline-none focus:ring-teal-500 focus:border-teal-500 sm:text-sm">
                                </div>
                                <div>
                                    <label for="feat1PredPower" class="block text-sm font-medium text-stone-700">Predictive Power (0-1, higher=better):</label>
                                    <input type="number" id="feat1PredPower" step="0.01" min="0" max="1" value="0.7" class="mt-1 block w-full px-3 py-2 border border-stone-300 rounded-md shadow-sm focus:outline-none focus:ring-teal-500 focus:border-teal-500 sm:text-sm">
                                </div>
                            </div>
                        </div>
                        <div>
                            <h4 class="text-lg font-semibold mb-3 text-teal-600">Feature 2 Details:</h4>
                             <div class="space-y-3">
                                <div>
                                    <label for="feat2Name" class="block text-sm font-medium text-stone-700">Name:</label>
                                    <input type="text" id="feat2Name" value="Feature B" class="mt-1 block w-full px-3 py-2 border border-stone-300 rounded-md shadow-sm focus:outline-none focus:ring-teal-500 focus:border-teal-500 sm:text-sm">
                                </div>
                                <div>
                                    <label for="feat2Missing" class="block text-sm font-medium text-stone-700">Missing Proportion (0-1, higher=worse):</label>
                                    <input type="number" id="feat2Missing" step="0.01" min="0" max="1" value="0.3" class="mt-1 block w-full px-3 py-2 border border-stone-300 rounded-md shadow-sm focus:outline-none focus:ring-teal-500 focus:border-teal-500 sm:text-sm">
                                </div>
                                <div>
                                    <label for="feat2TargetCorr" class="block text-sm font-medium text-stone-700">Target Correlation (0-1, higher=better):</label>
                                    <input type="number" id="feat2TargetCorr" step="0.01" min="0" max="1" value="0.6" class="mt-1 block w-full px-3 py-2 border border-stone-300 rounded-md shadow-sm focus:outline-none focus:ring-teal-500 focus:border-teal-500 sm:text-sm">
                                </div>
                                <div>
                                    <label for="feat2PredPower" class="block text-sm font-medium text-stone-700">Predictive Power (0-1, higher=better):</label>
                                    <input type="number" id="feat2PredPower" step="0.01" min="0" max="1" value="0.5" class="mt-1 block w-full px-3 py-2 border border-stone-300 rounded-md shadow-sm focus:outline-none focus:ring-teal-500 focus:border-teal-500 sm:text-sm">
                                </div>
                            </div>
                        </div>
                    </div>
                    <button id="calculateExplorer" class="w-full md:w-auto bg-teal-600 hover:bg-teal-700 text-white font-semibold py-2 px-6 rounded-lg shadow transition duration-150 ease-in-out">
                        Calculate Decision
                    </button>

                    <div id="explorerResult" class="mt-8">
                        <h4 class="text-lg font-semibold mb-3">Decision:</h4>
                        <table class="min-w-full divide-y divide-stone-200 border border-stone-300 rounded-md">
                            <thead class="bg-stone-50">
                                <tr>
                                    <th class="px-4 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Feature Name</th>
                                    <th class="px-4 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Composite Removal Score (Lower=Better)</th>
                                    <th class="px-4 py-3 text-left text-xs font-medium text-stone-500 uppercase tracking-wider">Decision</th>
                                </tr>
                            </thead>
                            <tbody class="bg-white divide-y divide-stone-200" id="explorerTableBody">
                                </tbody>
                        </table>
                    </div>
                </div>
            </section>
            
            <section id="advanced-tips" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">Advanced Considerations & Best Practices</h2>
                <p class="mb-4 leading-relaxed">
                    To make the most of this feature removal approach, consider these points:
                </p>
                <ul class="list-disc list-inside mb-6 pl-4 space-y-3 leading-relaxed bg-white p-6 rounded-lg shadow">
                    <li>
                        <strong class="text-teal-600">Performance Optimization:</strong> For very large datasets, calculating predictive power for each feature can be slow. Consider using vectorized operations, sampling, or tools like Numba/Cython if performance is critical.
                    </li>
                    <li>
                        <strong class="text-teal-600">Sensitivity Analysis & Threshold Tuning:</strong> The `corr_threshold` for grouping features and the weights for the composite score are key hyperparameters. Experiment with different values and use cross-validation to assess their impact on your specific model and data. There's no one-size-fits-all.
                    </li>
                    <li>
                        <strong class="text-teal-600">Handling Edge Cases:</strong> Be mindful of features with zero variance (constant values), as they can cause issues in calculations. The function should ideally handle these by assigning default scores. Also, consider how non-numeric data (like dates or free text) is handled; they might need preprocessing or exclusion.
                    </li>
                    <li>
                        <strong class="text-teal-600">Outliers:</strong> Extreme outliers can affect Pearson correlation and Min-Max scaling. If your data has significant outliers, explore robust scaling methods or outlier treatment before applying this function.
                    </li>
                    <li>
                        <strong class="text-teal-600">Data Understanding:</strong> Always start with a thorough understanding of your dataset—feature distributions, potential issues, and the nature of your target variable. This context is vital for interpreting the results of any automated feature selection process.
                    </li>
                </ul>
            </section>

            <section id="conclusion" class="content-section">
                <h2 class="text-3xl font-semibold mb-6 text-teal-700">Conclusion & Recommendations</h2>
                <p class="mb-4 leading-relaxed">
                    The multi-criteria approach for removing highly correlated features offers a more data-driven and nuanced solution than simpler methods. By considering missingness, target relevance, and individual predictive power, it aims to retain the most valuable features within correlated groups.
                </p>
                <p class="mb-4 leading-relaxed">Key recommendations for practitioners include:</p>
                <ul class="list-disc list-inside mb-4 pl-4 space-y-2 leading-relaxed bg-stone-50 p-6 rounded-lg shadow">
                    <li><strong>Understand Your Data:</strong> Deeply analyze your dataset before applying any automated selection.</li>
                    <li><strong>Experiment with Parameters:</strong> Tune the correlation threshold and score weights to fit your specific project needs.
                    </li>
                    <li><strong>Integrate into a Pipeline:</strong> Use this as an early preprocessing step in your machine learning workflow.</li>
                </ul>
                <p class="mb-4 leading-relaxed">
                    Future enhancements could involve more advanced imputation, alternative predictive power metrics, or even integrating model-based feature importance for a hybrid approach.
                </p>
                <p class="leading-relaxed">
                    This interactive guide aimed to provide a clear understanding of this optimal feature removal strategy. We hope it empowers you to build better, more efficient machine learning models!
                </p>
            </section>

        </main>
    </div>

    <script>
        const sections = document.querySelectorAll('.content-section');
        const navLinks = document.querySelectorAll('#sidebarNav a.sidebar-link');

        function setActiveSection(hash) {
            sections.forEach(section => {
                if ('#' + section.id === hash) {
                    section.classList.add('active');
                } else {
                    section.classList.remove('active');
                }
            });
            navLinks.forEach(link => {
                if (link.getAttribute('href') === hash) {
                    link.classList.add('active-link');
                } else {
                    link.classList.remove('active-link');
                }
            });
        }

        navLinks.forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const targetHash = this.getAttribute('href');
                window.location.hash = targetHash;
                setActiveSection(targetHash);
            });
        });
        
        // Initial section activation based on hash or default
        const currentHash = window.location.hash || '#overview';
        setActiveSection(currentHash);
        if (!window.location.hash) { // Ensure default is set in URL for consistency
            window.location.hash = '#overview';
        }


        // Chart.js initializations
        function createBarChart(canvasId, label, labels, data, backgroundColors, borderColors) {
            const ctx = document.getElementById(canvasId).getContext('2d');
            new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: labels,
                    datasets: [{
                        label: label,
                        data: data,
                        backgroundColor: backgroundColors,
                        borderColor: borderColors,
                        borderWidth: 1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: { beginAtZero: true, max: Math.max(...data, 0.5, 1.0) * 1.1 }, // Adjusted max for better visualization
                        x: { ticks: { callback: function(value, index, values) { const lbl = this.getLabelForValue(value); return lbl.length > 16 ? lbl.substring(0,16) + '...' : lbl; } } }
                    },
                    plugins: { legend: { display: true } }
                }
            });
        }

        // Metric Charts Data
        if (document.getElementById('missingDataChart')) {
            createBarChart('missingDataChart', 'Missing Proportion (Higher is worse)',
                ['Feature X', 'Feature Y', 'Feature Z'],
                [0.05, 0.22, 0.53],
                ['rgba(255, 159, 64, 0.5)', 'rgba(255, 159, 64, 0.5)', 'rgba(255, 159, 64, 0.5)'],
                ['rgba(255, 159, 64, 1)', 'rgba(255, 159, 64, 1)', 'rgba(255, 159, 64, 1)']
            );
        }

        if (document.getElementById('targetRelationshipChart')) {
            createBarChart('targetRelationshipChart', 'Normalized Target Relationship (Higher is better)',
                ['Feature X', 'Feature Y', 'Feature Z'],
                [0.85, 0.60, 0.25],
                ['rgba(75, 192, 192, 0.5)', 'rgba(75, 192, 192, 0.5)', 'rgba(75, 192, 192, 0.5)'],
                ['rgba(75, 192, 192, 1)', 'rgba(75, 192, 192, 1)', 'rgba(75, 192, 192, 1)']
            );
        }

        if (document.getElementById('predictivePowerChart')) {
            createBarChart('predictivePowerChart', 'Normalized Predictive Power (Higher is better)',
                ['Feature X', 'Feature Y', 'Feature Z'],
                [0.78, 0.65, 0.30],
                ['rgba(153, 102, 255, 0.5)', 'rgba(153, 102, 255, 0.5)', 'rgba(153, 102, 255, 0.5)'],
                ['rgba(153, 102, 255, 1)', 'rgba(153, 102, 255, 1)', 'rgba(153, 102, 255, 1)']
            );
        }

        // Composite Score Interactive Sliders & Table
        const missingWeightSlider = document.getElementById('missingWeight');
        const targetCorrWeightSlider = document.getElementById('targetCorrWeight');
        const liftWeightSlider = document.getElementById('liftWeight');
        
        const missingWeightValueSpan = document.getElementById('missingWeightValue');
        const targetCorrWeightValueSpan = document.getElementById('targetCorrWeightValue');
        const liftWeightValueSpan = document.getElementById('liftWeightValue');
        const compositeScoreTableBody = document.getElementById('compositeScoreTableBody');

        const exampleFeatures = [
            { name: 'Feature Alpha', normMissing: 0.1, normTargetCorr: 0.9, normPredPower: 0.85 },
            { name: 'Feature Beta', normMissing: 0.4, normTargetCorr: 0.7, normPredPower: 0.7 },
            { name: 'Feature Gamma', normMissing: 0.7, normTargetCorr: 0.3, normPredPower: 0.2 }
        ];

        function calculateAndDisplayCompositeScores() {
            let wMissing = parseFloat(missingWeightSlider.value);
            let wTargetCorr = parseFloat(targetCorrWeightSlider.value);
            let wLift = parseFloat(liftWeightSlider.value);

            missingWeightValueSpan.textContent = wMissing.toFixed(2);
            targetCorrWeightValueSpan.textContent = wTargetCorr.toFixed(2);
            liftWeightValueSpan.textContent = wLift.toFixed(2);

            const totalWeight = wMissing + wTargetCorr + wLift;
            if (totalWeight === 0) { // Avoid division by zero, though sliders prevent this unless all are 0
                 wMissing = 1/3; wTargetCorr = 1/3; wLift = 1/3; // Default if sum is 0
            } else { // Normalize weights
                wMissing = wMissing / totalWeight;
                wTargetCorr = wTargetCorr / totalWeight;
                wLift = wLift / totalWeight;
            }
            
            compositeScoreTableBody.innerHTML = ''; // Clear previous results
            exampleFeatures.forEach(feat => {
                const removalScore = (wMissing * feat.normMissing) +
                                     (wTargetCorr * (1 - feat.normTargetCorr)) +
                                     (wLift * (1 - feat.normPredPower));
                
                const row = `<tr>
                                <td class="px-4 py-3 whitespace-nowrap text-sm text-stone-700">${feat.name}</td>
                                <td class="px-4 py-3 whitespace-nowrap text-sm text-stone-700">${feat.normMissing.toFixed(2)}</td>
                                <td class="px-4 py-3 whitespace-nowrap text-sm text-stone-700">${feat.normTargetCorr.toFixed(2)}</td>
                                <td class="px-4 py-3 whitespace-nowrap text-sm text-stone-700">${feat.normPredPower.toFixed(2)}</td>
                                <td class="px-4 py-3 whitespace-nowrap text-sm font-medium text-teal-700">${removalScore.toFixed(3)}</td>
                             </tr>`;
                compositeScoreTableBody.innerHTML += row;
            });
        }
        
        if (missingWeightSlider && targetCorrWeightSlider && liftWeightSlider) {
            [missingWeightSlider, targetCorrWeightSlider, liftWeightSlider].forEach(slider => {
                slider.addEventListener('input', calculateAndDisplayCompositeScores);
            });
            calculateAndDisplayCompositeScores(); // Initial calculation
        }

        // Interactive Explorer Logic
        const calculateExplorerBtn = document.getElementById('calculateExplorer');
        const explorerTableBody = document.getElementById('explorerTableBody');

        if (calculateExplorerBtn) {
            calculateExplorerBtn.addEventListener('click', function() {
                const feat1 = {
                    name: document.getElementById('feat1Name').value || 'Feature 1',
                    missing: parseFloat(document.getElementById('feat1Missing').value),
                    targetCorr: parseFloat(document.getElementById('feat1TargetCorr').value),
                    predPower: parseFloat(document.getElementById('feat1PredPower').value)
                };
                const feat2 = {
                    name: document.getElementById('feat2Name').value || 'Feature 2',
                    missing: parseFloat(document.getElementById('feat2Missing').value),
                    targetCorr: parseFloat(document.getElementById('feat2TargetCorr').value),
                    predPower: parseFloat(document.getElementById('feat2PredPower').value)
                };

                let wM = parseFloat(missingWeightSlider.value);
                let wTC = parseFloat(targetCorrWeightSlider.value);
                let wPP = parseFloat(liftWeightSlider.value);
                const totalW = wM + wTC + wPP;
                 if (totalW === 0) { wM=1/3; wTC=1/3; wPP=1/3; }
                 else { wM /= totalW; wTC /= totalW; wPP /= totalW; }


                const score1 = (wM * feat1.missing) + (wTC * (1 - feat1.targetCorr)) + (wPP * (1 - feat1.predPower));
                const score2 = (wM * feat2.missing) + (wTC * (1 - feat2.targetCorr)) + (wPP * (1 - feat2.predPower));

                explorerTableBody.innerHTML = '';
                let decision1 = '', decision2 = '';
                if (score1 <= score2) {
                    decision1 = '<span class="pill pill-keep">Keep</span>';
                    decision2 = '<span class="pill pill-remove">Remove</span>';
                } else {
                    decision1 = '<span class="pill pill-remove">Remove</span>';
                    decision2 = '<span class="pill pill-keep">Keep</span>';
                }
                
                explorerTableBody.innerHTML += `
                    <tr>
                        <td class="px-4 py-3 whitespace-nowrap text-sm text-stone-700">${feat1.name}</td>
                        <td class="px-4 py-3 whitespace-nowrap text-sm font-medium text-teal-700">${score1.toFixed(3)}</td>
                        <td class="px-4 py-3 whitespace-nowrap text-sm">${decision1}</td>
                    </tr>
                    <tr>
                        <td class="px-4 py-3 whitespace-nowrap text-sm text-stone-700">${feat2.name}</td>
                        <td class="px-4 py-3 whitespace-nowrap text-sm font-medium text-teal-700">${score2.toFixed(3)}</td>
                        <td class="px-4 py-3 whitespace-nowrap text-sm">${decision2}</td>
                    </tr>
                `;
            });
        }

    </script>
</body>
</html>


